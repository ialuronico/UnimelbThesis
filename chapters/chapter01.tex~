\chapter{Introduction}\label{ch:introduction}

Dependency measures are extensively used in data mining and machine learning to assess the amount of dependency between the variables in a data set. Variable selection for classification and regression in a supervised learning task is one of the main applications of dependency measures: they are used to identify and rank the most predictive variables to the target class \citep{Guyon2003}. They are also fundamental for decision tree induction \citep{Crimisini2012}. During the decision tree induction process, the most predictive variable to the target class is identified according to a dependency measure in order to meaningfully split internal nodes in the tree. Even the prediction accuracy of a classification model can be seen as a dependency measure: it assesses the dependency between the predicted class and actual class \citep{Witten2011}.

Dependency measures are also extensively used in unsupervised learning in the clustering community \citep{Aggarwal2013}. In this case, their main application is external clustering validation \citep{Hubert85}: clustering solutions obtained using different clustering algorithms are compared to a ground truth clustering of reference. Furthermore, dependency measures based on correlation are sometimes the core technique of clustering algorithms \citep{Bohm2004} and they are used to compare clusterings in many other applications. For example, high dimensional data sets allow multiple valid clusterings. These different clusterings, also called different views, can be compared and analyzed with dependency measures \citep{Muller2013}. Dependency measures can guide the generation of alternative clustering solutions and compare their diversity \citep{Kontonasios2013,Dang2015}, can be used to assess the similarity between clusterings in a cluster ensemble \citep{Strehl2003}, and can be used to explore the clustering space using results from the Meta-Clustering algorithm \citep{Caruana2006, Lei2014} when the task it to find similar/dissimilar clusterings from a query one. 

Dependency measures between variables are also used for exploratory analysis in machine learning and data mining. There exist applications on inference of cellular, metabolic, gene regulatory, biological, and signaling networks \citep{Scholkopf2004,Villaverde2013}. In this case, the dependency measure is used for reverse engineering the network of interaction between variables. Moreover, dependency measures are also extensively employed to compare neural time-series \citep{Cohen2014} to explore the interaction between different areas of the brain.

There are three important applications of dependency measures between variables \citep{Reimherr2013}:
\begin{description}
\item[Detection:] Test for the presence of dependency. It is useful in case the dependency is not trivial or even its existence is questionable. For example, assess if there exists any dependency between bacterial species that colonize the gut of mammals~\citep{Reshef2011} or identify genes whose expression level oscillates \citep{Sugiyama2013};
\item[Quantification:] Summarization of the amount of dependency in an interpretable fashion. In this case, the presence of dependency has already been established and the target is to provide insights on the amount of dependency. The value of the dependency measure must be meaningful and must range in a predefined interval to aid intuition. For example, when the squared Pearson's correlation coefficient is used to quantify the amount of linear dependency between two variables. This has a clear interpretation and the possible values range in $[0,1]$;
\item[Ranking:] Sort the relationships of different variables based on the strength of their dependency. In this case, it is less important to obtain an interpretable number out of a dependency measure given that the user is interested only in the accurate ranking. For example, when a dependency measure is used to rank predictive variables to the target class during the decision tree induction phase.
\end{description}
However even though a dependency measure has nice theoretical properties, dependencies are estimated on a finite data set through an estimator and the tasks above become challenging. A dependency measure estimator is a random variable that can attain different possible values. When \emph{detecting} dependencies, the estimates of a real dependency have to be easily discriminated from the estimates under lack of dependency. If the range of possible values under real dependency and the range of possible values under lack of dependency overlap, a dependency measure has low detection power \citep{Simon2014}. When \emph{quantifying} dependency the estimated values are dependent on the data sample used for the estimation. Dependency measure estimates can be high because of chance when the sample size is small, e.g., because of missing values, or when the dependency is estimated between categorical variables with many categories. Therefore, dependency estimates cannot be meaningfully compared between data sets and this is particularly detrimental if the dependency measure must have a clear interpretation. Moreover, this issue causes problems when \emph{ranking} dependencies. Estimates take into account factors which are not dependent only on the strength of the dependency, and this causes biases in ranking variables with missing values or categorical variables with many categories.

The body of work presented in this thesis contributes to the development of statistical techniques to adjusting and designing dependency measure estimators to solve these challenges.

\section{Thesis Overview}

Chapter \ref{ch:background} defines fundamental concepts in machine learning and the different types of dependency measures available in literature as well as examples for their application scenarios.

In Chapter \ref{ch:ranknoise}, we discuss the design of a dependency measure between continuous variables based on the estimation of the normalized mutual information \citep{Cover2012}. In order to estimate the normalized mutual information between numerical variables, we employ random discretization grids. We show that this helps in reducing the variance while minimizing the impact on bias. This is particularly useful when \emph{detecting} and \emph{ranking} noisy dependencies: the estimates obtained on different relationships have small variance, therefore the range of possible values under one relationship has small overlap with the range of estimates of a different relationship; this allows to better discriminate strong from weaker dependencies between variables. Moreover, we show that a small estimation variance is likely to be more useful than a smaller bias if this bias is systematic: indeed systematic biases in estimation cancel each other out when ranking dependencies and when a measure is employed to detect the presence of a dependency. As a result of this theory, we define the Randomized Information Coefficient (RIC) and we demonstrate its effectiveness for detecting noisy dependencies and ranking dependencies for the applications of genetic network inference and feature selection for regression. Across these tasks, RIC shows to be competitive to other 15 state-of-the-art measures.

In Chapter \ref{ch:adjdep}, we introduce a framework to adjust dependency measure estimates for chance. Dependency measure estimates are estimated on a data set and they can be high because of chance when the sample size is small, or when the dependency is estimated between categorical variables with many categories. These problems do not allow to meaningfully compare dependency estimates across data sets. If the value of a dependency measure must have a clear interpretation, this causes problems in particular when \emph{quantifying} dependencies. Moreover, these biases cause problems when \emph{ranking} dependencies. Therefore, firstly we formally define the desiderata of dependency measure estimates such as unbiasedness for \emph{quantification} and for \emph{ranking} with regards to the data set used. Secondly, in order to achieve these properties, we propose a general framework to adjust dependency measures. Then, we apply our adjustments to dependency measures between numerical variables such as the Pearson's correlation to detect linear dependencies and the Maximal Information Coefficient (MIC) \citep{Reshef2011} and show how to achieve higher interpretability in \emph{quantification} and higher accuracy in \emph{ranking}. Ultimately, we demonstrate that our approach is effective to rank variables during the splitting procedure in random forests \citep{Breiman2001} where a dependency measure between categorical variables is employed.

In Chapter \ref{ch:adjinfo}, we discuss adjustments for chance of dependency measures between clusterings of the same data set, i.e., categorical variables. In order to adjust dependency measures, we compute summary statistics such as the expected value and the standard deviation of the distribution of a dependency measure estimator under the null hypothesis of independence between variables. In most of the cases the distribution can be approximated by the computation of the measure under independence via Monte Carlo permutations. Nonetheless, we show that when the dependency measure is computed between partitions/clusterings of the same data set, we can analytically derive expected value and standard deviation for a class of measures we name $\mathcal{L}_\phi$. This allows us to generalize adjustments for clustering comparison measures. In particular, the Adjusted Rand Index (ARI) \citep{Hubert85} based on pair-counting, and the Adjusted Mutual Information (AMI) \citep{Nguyen2009} based on Shannon information theory are very popular in the clustering community. Nonetheless it is an open problem as to what are the best application scenarios for each measure and guidelines in the literature for their usage are sparse, with the result that users often resort to using both. With our theory, we aim to bridge the gap between adjustment of measures based on pair-counting and measures based on information theory. Moreover, we show that expected value and standard deviation can also be computed analytically for a more general class of measures we name $\mathcal{N}_\phi$ under the assumption of large sample size.
%$\mathcal{N}_\phi$ includes $\mathcal{L}_\phi$ and includes as special members the Jaccard similarity. 
 
Finally, in Chapter \ref{ch:conclusions}, we present the conclusions of our research and propose avenues for future work.
